# Cache 替换策略实验报告

计72 马川 2017011409

## 算法简介

本次实验中，我采用了自己设计的 PLRU 算法进行 Cache 替换。该算法为 LIRS 的简化实现版本，可以有效地提升数据大量循环访问时的命中率。同时对 LIRS 算法进行了部分改动，使得其可以更好地适应两个距离较大的工作集上 Cache。

### LRU 算法的问题

### FRU 算法的问题

### LIRS 算法

### PLRU 算法

本次实验中使用的 PLRU 算法全称为 Priviledge LRU，其在 LRU-2 算法之上增加一位特权位(Priviledge Bit)，并且结合 LIRS 的替换策略形成了一种新的算法。

* PLRU 算法的理念是将缓存分为两个部分，一个部分称为特权区，用于存放被频繁访问的数据，并且使这些数据很难被替换
* 每一个 Cache Line 都具有一个特权位，被标记为“特权”的 Cache Line 被认为放置在一个栈中。没有标记为特权的块被认为放置在一个队列中，这个队列在后续的计算中有最小的长度限制，这样可以留出至少一个 Cache 块的空间储存很少重复访问的数据，降低缓存抖动的发生概率。

* 根据是否特权位是否为1，可以将每个缓存块标记为高特权块(HPB)和低特权块(LPB)。其中，高特权块更难被替换，低特权级块更容易被替换。低特权级块根据先进先出的原则进行替换，当低特权级块被再次访问时，其会提升为高特权块。所有的高特权级块根据 LRU 的规则储存在栈中，如果高特权级的块被命中，则将其 LRU 的值清零。高特权级块还会进行“衰变”操作：当一个高特权级块的 LRU 值足够大时，可以认为这个块在短时间内不会再次访问，此时其会从高特权级块降低为低特权级块，这可以帮助算法更快地切换到另一个工作集上。

* 等待队列的长度与特权栈的长度之和为组相联的组数

## 算法原理说明

PLRU 算法的核心体现在特权级的升降上。我称所有低特权级的块组成的队列为“等待队列”(Waiting List)，所有高特权级的块组成的栈称之为“特权栈”。特权栈的作用是利用空间局部性，储存经常被访问的数据块，同时令这些过去经常被访问的块很难被替换，使得缓存在间隔较长的集中访问上表现更好。等待队列则是

## 算法适用环境分析

## 算法测试结果展示
